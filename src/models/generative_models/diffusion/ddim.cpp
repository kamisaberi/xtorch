#include "include/models/generative_models/diffusion/ddim.h"


using namespace std;

//DDIM GROK


#include <torch/torch.h>
#include <opencv2/opencv.hpp>
#include <opencv2/imgproc.hpp>
#include <filesystem>
#include <iostream>
#include <vector>
#include <string>

// Simplified Text Encoder for Conditioning
struct TextEncoderImpl : torch::nn::Module {
    TextEncoderImpl(int text_dim = 256, int text_seq_len = 16) {
        text_embed = register_module("text_embed", torch::nn::Embedding(10000, text_dim));
        text_transformer = register_module("text_transformer", torch::nn::TransformerEncoder(
            torch::nn::TransformerEncoderOptions(
                torch::nn::TransformerEncoderLayerOptions(text_dim, 4, text_dim * 4).dropout(0.1), 2
            )
        ));
        text_pos_embed = register_parameter("text_pos_embed", torch::randn({text_seq_len, text_dim}));
        norm = register_module("norm", torch::nn::LayerNorm(torch::nn::LayerNormOptions({text_dim})));
    }

    torch::Tensor forward(torch::Tensor text) {
        auto emb = text_embed->forward(text) + text_pos_embed; // [batch, text_seq_len, text_dim]
        emb = text_transformer->forward(emb); // [batch, text_seq_len, text_dim]
        return norm->forward(emb.mean(1)); // [batch, text_dim]
    }

    torch::nn::Embedding text_embed{nullptr};
    torch::nn::TransformerEncoder text_transformer{nullptr};
    torch::Tensor text_pos_embed;
    torch::nn::LayerNorm norm{nullptr};
};
TORCH_MODULE(TextEncoder);

// Simplified U-Net for Diffusion
struct UNetImpl : torch::nn::Module {
    UNetImpl(int dim = 64, int cond_dim = 256) {
        init_conv = register_module("init_conv", torch::nn::Conv2d(
            torch::nn::Conv2dOptions(1, dim, 3).padding(1)));
        down1 = register_module("down1", torch::nn::Conv2d(
            torch::nn::Conv2dOptions(dim, dim * 2, 4).stride(2).padding(1)));
        down2 = register_module("down2", torch::nn::Conv2d(
            torch::nn::Conv2dOptions(dim * 2, dim * 4, 4).stride(2).padding(1)));
        mid = register_module("mid", torch::nn::Conv2d(
            torch::nn::Conv2dOptions(dim * 4, dim * 4, 3).padding(1)));
        up1 = register_module("up1", torch::nn::ConvTranspose2d(
            torch::nn::ConvTranspose2dOptions(dim * 4, dim * 2, 4).stride(2).padding(1).output_padding(1)));
        up2 = register_module("up2", torch::nn::ConvTranspose2d(
            torch::nn::ConvTranspose2dOptions(dim * 2, dim, 4).stride(2).padding(1).output_padding(1)));
        final_conv = register_module("final_conv", torch::nn::Conv2d(
            torch::nn::Conv2dOptions(dim, 1, 3).padding(1)));
        time_embed = register_module("time_embed", torch::nn::Linear(1000, dim * 4));
        cond_embed = register_module("cond_embed", torch::nn::Linear(cond_dim, dim * 4));
        relu = register_module("relu", torch::nn::ReLU());
    }

    torch::Tensor forward(torch::Tensor x, torch::Tensor t, torch::Tensor cond) {
        auto time_emb = relu->forward(time_embed->forward(t)).view({-1, 64 * 4, 1, 1}); // [batch, dim*4, 1, 1]
        auto cond_emb = relu->forward(cond_embed->forward(cond)).view({-1, 64 * 4, 1, 1}); // [batch, dim*4, 1, 1]
        x = relu->forward(init_conv->forward(x)); // [batch, dim, 28, 28]
        auto h1 = relu->forward(down1->forward(x)); // [batch, dim*2, 14, 14]
        auto h2 = relu->forward(down2->forward(h1)); // [batch, dim*4, 7, 7]
        auto h3 = relu->forward(mid->forward(h2)) + time_emb + cond_emb; // [batch, dim*4, 7, 7]
        auto h4 = relu->forward(up1->forward(h3)); // [batch, dim*2, 14, 14]
        auto h5 = relu->forward(up2->forward(h4)); // [batch, dim, 28, 28]
        return final_conv->forward(h5); // [batch, 1, 28, 28]
    }

    torch::nn::Conv2d init_conv{nullptr}, down1{nullptr}, down2{nullptr}, mid{nullptr};
    torch::nn::ConvTranspose2d up1{nullptr}, up2{nullptr}, final_conv{nullptr};
    torch::nn::Linear time_embed{nullptr}, cond_embed{nullptr};
    torch::nn::ReLU relu{nullptr};
};
TORCH_MODULE(UNet);

// DDIM Model with Classifier-Free Guidance
struct DDIMImpl : torch::nn::Module {
    DDIMImpl(int dim = 64, int cond_dim = 256, int timesteps = 1000, int sample_steps = 50)
        : timesteps_(timesteps), sample_steps_(sample_steps) {
        unet = register_module("unet", UNet(dim, cond_dim));
        // DDIM sampling schedule
        auto tau = torch::linspace(0, timesteps_ - 1, sample_steps_ + 1).to(torch::kInt64);
        tau = tau.slice(0, 1, sample_steps_ + 1); // Skip t=0
        tau_ = tau;
    }

    torch::Tensor forward(torch::Tensor x, torch::Tensor t, torch::Tensor cond, torch::Tensor cond_drop = torch::zeros({})) {
        if (!cond_drop.empty()) { // Classifier-free guidance
            auto null_cond = torch::zeros_like(cond);
            auto cond_output = unet->forward(x, t, cond);
            auto null_output = unet->forward(x, t, null_cond);
            return null_output + 3.0 * (cond_output - null_output); // Guidance scale = 3.0
        }
        return unet->forward(x, t, cond);
    }

    torch::Tensor sample(torch::Tensor cond, torch::Device device, float eta = 0.0) {
        torch::NoGradGuard no_grad;
        auto x = torch::randn({1, 1, 28, 28}, device); // Start with noise
        auto alpha_t = alphas_cumprod_.to(device);
        auto alpha_t_prev = torch::cat({torch::ones({1}), alphas_cumprod_.slice(0, 0, timesteps_ - 1)}, 0).to(device);

        for (int i = sample_steps_ - 1; i >= 0; --i) {
            auto t = torch::full({1}, tau_[i].item<int64_t>(), torch::kInt64, device);
            auto t_prev = i > 0 ? torch::full({1}, tau_[i - 1].item<int64_t>(), torch::kInt64, device)
                                : torch::zeros({1}, torch::kInt64, device);
            auto pred_noise = forward(x, t, cond);
            auto alpha_t_val = alpha_t[t.item<int64_t>()];
            auto alpha_t_prev_val = alpha_t_prev[t.item<int64_t>()];
            auto sigma_t = eta * torch::sqrt((1 - alpha_t_prev_val) / (1 - alpha_t_val) * (1 - alpha_t_val / alpha_t_prev_val));
            auto pred_x0 = (x - torch::sqrt(1 - alpha_t_val) * pred_noise) / torch::sqrt(alpha_t_val);
            x = torch::sqrt(alpha_t_prev_val) * pred_x0 +
                torch::sqrt(1 - alpha_t_prev_val - sigma_t * sigma_t) * pred_noise +
                sigma_t * torch::randn_like(x);
        }
        return x.clamp(0, 1);
    }

    void set_diffusion_schedule(torch::Tensor betas) {
        alphas_cumprod_ = torch::cumprod(1.0 - betas, 0);
    }

    int timesteps_, sample_steps_;
    torch::Tensor tau_, alphas_cumprod_;
    UNet unet{nullptr};
};
TORCH_MODULE(DDIM);

// Custom Dataset for Grayscale Images and Text
struct TextImageDataset : torch::data::Dataset<TextImageDataset> {
    TextImageDataset(const std::string& img_dir, const std::vector<std::string>& texts)
        : texts_(texts) {
        for (const auto& entry : std::filesystem::directory_iterator(img_dir)) {
            if (entry.path().extension() == ".jpg" || entry.path().extension() == ".png") {
                image_paths_.push_back(entry.path().string());
            }
        }
    }

    torch::data::Example<> get(size_t index) override {
        cv::Mat image = cv::imread(image_paths_[index % image_paths_.size()], cv::IMREAD_GRAYSCALE);
        if (image.empty()) {
            throw std::runtime_error("Failed to load image: " + image_paths_[index % image_paths_.size()]);
        }
        image.convertTo(image, CV_32F, 1.0 / 255.0);
        torch::Tensor img_tensor = torch::from_blob(image.data, {1, image.rows, image.cols}, torch::kFloat32);
        torch::Tensor text_tensor = torch::randint(0, 10000, {16}, torch::kInt64); // Mock text
        return {img_tensor, text_tensor};
    }

    torch::optional<size_t> size() const override {
        return image_paths_.size();
    }

    std::vector<std::string> image_paths_, texts_;
};

// Diffusion utilities
struct DiffusionUtils {
    DiffusionUtils(int timesteps) : timesteps_(timesteps) {
        betas = torch::linspace(1e-4, 0.02, timesteps).to(torch::kFloat);
        alphas = 1.0 - betas;
        alphas_cumprod = torch::cumprod(alphas, 0);
        sqrt_alphas_cumprod = torch::sqrt(alphas_cumprod);
        sqrt_one_minus_alphas_cumprod = torch::sqrt(1.0 - alphas_cumprod);
    }

    torch::Tensor add_noise(torch::Tensor x, torch::Tensor t) {
        auto sqrt_alpha = sqrt_alphas_cumprod.index({t}).view({-1, 1, 1, 1});
        auto sqrt_one_minus_alpha = sqrt_one_minus_alphas_cumprod.index({t}).view({-1, 1, 1, 1});
        auto noise = torch::randn_like(x);
        return sqrt_alpha * x + sqrt_one_minus_alpha * noise;
    }

    torch::Tensor sample_timesteps(int batch_size) {
        return torch::randint(0, timesteps_, {batch_size}, torch::kInt64);
    }

    torch::Tensor get_betas() { return betas; }

    int timesteps_;
    torch::Tensor betas, alphas, alphas_cumprod, sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod;
};

int main() {
    try {
        // Set device
        torch::Device device(torch::cuda::is_available() ? torch::kCUDA : torch::kCPU);
        std::cout << "Using device: " << (device.is_cuda() ? "CUDA" : "CPU") << std::endl;

        // Initialize models
        TextEncoder text_encoder(256, 16);
        DDIM ddim(64, 256, 1000, 50); // 50 sampling steps
        text_encoder->to(device);
        ddim->to(device);

        // Diffusion utilities
        DiffusionUtils diffusion_utils(1000);
        ddim->set_diffusion_schedule(diffusion_utils.get_betas());

        // Optimizers
        torch::optim::Adam text_optimizer(text_encoder->parameters(), torch::optim::AdamOptions(0.0003));
        torch::optim::Adam ddim_optimizer(ddim->parameters(), torch::optim::AdamOptions(0.0003));

        // Load dataset
        std::vector<std::string> mock_texts = {"digit", "number", "image"};
        auto dataset = TextImageDataset("./data/images", mock_texts)
            .map(torch::data::transforms::Stack<>());
        auto data_loader = torch::data::make_data_loader(
            dataset, torch::data::DataLoaderOptions().batch_size(32).workers(2));

        // Training loop (DDPM-like training)
        text_encoder->train();
        ddim->train();
        for (int epoch = 0; epoch < 20; ++epoch) {
            float total_loss = 0.0;
            int batch_count = 0;

            for (auto& batch : *data_loader) {
                auto images = batch.data.to(device);
                auto text = batch.target.to(device, torch::kInt64);
                auto t = diffusion_utils.sample_timesteps(images.size(0)).to(device);
                auto cond_drop = torch::rand({images.size(0)}) < 0.15; // 15% dropout for guidance

                // Text encoder
                text_optimizer.zero_grad();
                auto text_emb = text_encoder->forward(text);

                // DDIM (trained with DDPM loss)
                ddim_optimizer.zero_grad();
                auto noisy_images = diffusion_utils.add_noise(images, t);
                auto pred_noise = ddim->forward(noisy_images, t, text_emb, cond_drop.to(device));
                auto diffusion_loss = torch::nn::functional::mse_loss(pred_noise, images);
                diffusion_loss.backward();
                ddim_optimizer.step();

                total_loss += diffusion_loss.item<float>();
                batch_count++;
            }

            std::cout << "Epoch " << epoch + 1 << ", Loss: " << total_loss / batch_count << std::endl;
        }

        // Save models
        torch::save(text_encoder, "text_encoder_ddim.pt");
        torch::save(ddim, "ddim.pt");
        std::cout << "Models saved as text_encoder_ddim.pt and ddim.pt" << std::endl;

        // Inference example (DDIM sampling)
        text_encoder->eval();
        ddim->eval();
        torch::Tensor text_input = torch::randint(0, 10000, {1, 16}, torch::kInt64).to(device);
        auto text_emb = text_encoder->forward(text_input);
        auto generated = ddim->sample(text_emb, device, 0.0); // eta=0 for deterministic DDIM
       生成的 = generated.squeeze().to(torch::kCPU);
        cv::Mat output(28, 28, CV_32F, generated.data_ptr<float>());
        output.convertTo(output, CV_8U, 255.0);
        cv::imwrite("generated_ddim_image.jpg", output);
        std::cout << "Generated image saved as generated_ddim_image.jpg" << std::endl;

    } catch (const std::exception& e) {
        std::cerr << "Error: " << e.what() << std::endl;
        return -1;
    }

    return 0;
}

namespace xt::models
{
    DDIM::DDIM(int num_classes, int in_channels)
    {
    }

    DDIM::DDIM(int num_classes, int in_channels, std::vector<int64_t> input_shape)
    {
    }

    void DDIM::reset()
    {
    }

    auto DDIM::forward(std::initializer_list<std::any> tensors) -> std::any
    {
        std::vector<std::any> any_vec(tensors);

        std::vector<torch::Tensor> tensor_vec;
        for (const auto& item : any_vec)
        {
            tensor_vec.push_back(std::any_cast<torch::Tensor>(item));
        }

        torch::Tensor x = tensor_vec[0];

        return x;
    }
}
